# PostgreSQLæ€§èƒ½ä¼˜åŒ–ï¼šä»æ…¢æŸ¥è¯¢åˆ°æ¯«ç§’çº§å“åº”

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ

ä¸Šä¸ªæœˆæˆ‘ä»¬çš„ä¸»æŸ¥è¯¢ä»3.2ç§’ä¼˜åŒ–åˆ°äº†8msï¼Œæ•´æ•´æå‡äº†400å€ã€‚è¿™æ¬¡ä¼˜åŒ–è®©æˆ‘æ·±åˆ»ç†è§£äº†æ•°æ®åº“æ€§èƒ½è°ƒä¼˜çš„ç²¾é«“ã€‚

**æˆ‘çš„ç»å†ï¼š**

æœ€åˆæˆ‘ä»¥ä¸ºåŠ å‡ ä¸ªç´¢å¼•å°±èƒ½è§£å†³é—®é¢˜ï¼Œç»“æœæŸ¥è¯¢åè€Œæ›´æ…¢äº†ã€‚

åæ¥å‘ç°ç´¢å¼•ç­–ç•¥ã€æŸ¥è¯¢è®¡åˆ’ã€ç»Ÿè®¡ä¿¡æ¯ã€è¿æ¥æ± é…ç½®ç­‰éƒ½ä¼šå½±å“æ€§èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹ã€‚

ç°åœ¨æˆ‘çš„çœ‹æ³•æ˜¯ï¼šPostgreSQLæ€§èƒ½ä¼˜åŒ–éœ€è¦ç†è§£æŸ¥è¯¢æ‰§è¡Œå™¨çš„å·¥ä½œåŸç†ï¼Œè€Œä¸æ˜¯ç›²ç›®æ·»åŠ ç´¢å¼•ã€‚

**æ ¸å¿ƒæ¦‚å¿µè§£æï¼š**

ç´¢å¼•é€‰æ‹©åŸåˆ™ï¼š
1. B-Treeç´¢å¼•ï¼šé€‚ç”¨äºç­‰å€¼æŸ¥è¯¢å’ŒèŒƒå›´æŸ¥è¯¢ï¼ˆé»˜è®¤ï¼‰
2. GINç´¢å¼•ï¼šé€‚ç”¨äºå…¨æ–‡æœç´¢ã€JSONæŸ¥è¯¢ã€æ•°ç»„
3. GiSTç´¢å¼•ï¼šé€‚ç”¨äºåœ°ç†æ•°æ®ã€èŒƒå›´ç±»å‹
4. BRINç´¢å¼•ï¼šé€‚ç”¨äºå¤§è¡¨çš„æ—¶åºæ•°æ®

æŸ¥è¯¢ä¼˜åŒ–å™¨ï¼š
- åŸºäºæˆæœ¬æ¨¡å‹ï¼ˆCost-Based Optimizerï¼‰
- ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ä¼°ç®—è¡Œæ•°
- é€‰æ‹©æœ€ä¼˜æ‰§è¡Œè®¡åˆ’

è¿æ¥ç±»å‹ï¼š
- Nested Loopï¼šå°è¡¨é©±åŠ¨å¤§è¡¨
- Hash Joinï¼šç­‰å€¼è¿æ¥å¤§è¡¨
- Merge Joinï¼šå·²æ’åºæ•°æ®çš„è¿æ¥

**å®æˆ˜æ¡ˆä¾‹ï¼š**

åœºæ™¯ï¼šç”µå•†è®¢å•ç³»ç»Ÿï¼Œéœ€è¦æŸ¥è¯¢ç”¨æˆ·æœ€è¿‘30å¤©çš„è®¢å•ç»Ÿè®¡ï¼ŒåŒ…å«å•†å“è¯¦æƒ…ã€æ”¯ä»˜ä¿¡æ¯ç­‰ã€‚

é—®é¢˜ï¼šæŸ¥è¯¢è€—æ—¶3.2ç§’ï¼Œä¸¥é‡å½±å“ç”¨æˆ·ä½“éªŒï¼Œæ•°æ®åº“CPUä½¿ç”¨ç‡90%ã€‚

åŸå§‹æŸ¥è¯¢ï¼š
```sql
SELECT
    o.order_id,
    o.created_at,
    u.username,
    u.email,
    p.product_name,
    p.price,
    pay.payment_method,
    pay.paid_at
FROM orders o
JOIN users u ON o.user_id = u.user_id
JOIN order_items oi ON o.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
LEFT JOIN payments pay ON o.order_id = pay.order_id
WHERE o.created_at >= NOW() - INTERVAL '30 days'
AND o.status = 'completed'
ORDER BY o.created_at DESC
LIMIT 100;
```

ä¼˜åŒ–æ­¥éª¤ï¼š

1. åˆ†ææŸ¥è¯¢è®¡åˆ’ï¼š
```sql
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
[ä¸Šè¿°æŸ¥è¯¢];
```

å‘ç°é—®é¢˜ï¼š
- ordersè¡¨å…¨è¡¨æ‰«æï¼ˆSeq Scanï¼‰ï¼Œæ‰«æ200ä¸‡è¡Œ
- æ²¡æœ‰ä½¿ç”¨ç´¢å¼•
- Hash Joinæ¶ˆè€—å¤§é‡å†…å­˜

2. åˆ›å»ºå¤åˆç´¢å¼•ï¼š
```sql
-- è¦†ç›–WHEREå’ŒORDER BYæ¡ä»¶
CREATE INDEX idx_orders_status_created
ON orders(status, created_at DESC)
WHERE status = 'completed';

-- åŠ é€ŸJOIN
CREATE INDEX idx_order_items_order_id
ON order_items(order_id)
INCLUDE (product_id);

CREATE INDEX idx_payments_order_id
ON payments(order_id);
```

3. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼š
```sql
ANALYZE orders;
ANALYZE order_items;
ANALYZE products;
ANALYZE payments;
```

4. ä¼˜åŒ–æŸ¥è¯¢å†™æ³•ï¼ˆä½¿ç”¨CTEåˆ†è§£å¤æ‚æŸ¥è¯¢ï¼‰ï¼š
```sql
WITH recent_orders AS (
    SELECT
        order_id,
        user_id,
        created_at,
        status
    FROM orders
    WHERE status = 'completed'
    AND created_at >= NOW() - INTERVAL '30 days'
    ORDER BY created_at DESC
    LIMIT 100
)
SELECT
    ro.order_id,
    ro.created_at,
    u.username,
    u.email,
    p.product_name,
    p.price,
    pay.payment_method,
    pay.paid_at
FROM recent_orders ro
JOIN users u ON ro.user_id = u.user_id
JOIN order_items oi ON ro.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
LEFT JOIN payments pay ON ro.order_id = pay.order_id
ORDER BY ro.created_at DESC;
```

ç»“æœï¼š
- æŸ¥è¯¢æ—¶é—´ï¼š3200ms â†’ 8msï¼ˆæå‡400å€ï¼‰
- æ‰«æè¡Œæ•°ï¼š2,000,000 â†’ 342
- å†…å­˜ä½¿ç”¨ï¼šå‡å°‘85%
- CPUä½¿ç”¨ç‡ï¼šä»90% â†’ 15%

**æŠ€æœ¯è¦ç‚¹ï¼š**

â€¢ ä½¿ç”¨EXPLAIN ANALYZEåˆ†ææŸ¥è¯¢ï¼š
```sql
-- æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’å’Œå®é™…æ‰§è¡Œæ—¶é—´
EXPLAIN (ANALYZE, BUFFERS, VERBOSE) SELECT ...;

-- é‡ç‚¹å…³æ³¨ï¼š
-- 1. Seq Scanï¼ˆå…¨è¡¨æ‰«æï¼‰- éœ€è¦æ·»åŠ ç´¢å¼•
-- 2. rowsï¼ˆä¼°ç®—è¡Œæ•° vs å®é™…è¡Œæ•°ï¼‰- ç»Ÿè®¡ä¿¡æ¯æ˜¯å¦å‡†ç¡®
-- 3. buffersï¼ˆç¼“å†²åŒºå‘½ä¸­ç‡ï¼‰- æ˜¯å¦éœ€è¦å¢åŠ shared_buffers
```

â€¢ åˆ†åŒºè¡¨ä¼˜åŒ–å¤§æ•°æ®æŸ¥è¯¢ï¼š
```sql
-- æŒ‰æ—¶é—´åˆ†åŒºï¼ˆPostgreSQL 16ï¼‰
CREATE TABLE orders (
    order_id BIGINT,
    created_at TIMESTAMP,
    status VARCHAR(20)
) PARTITION BY RANGE (created_at);

CREATE TABLE orders_2025_01 PARTITION OF orders
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE orders_2025_02 PARTITION OF orders
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

â€¢ è¿æ¥æ± é…ç½®ï¼ˆä½¿ç”¨PgBouncerï¼‰ï¼š
```ini
# pgbouncer.ini
[databases]
mydb = host=localhost port=5432 dbname=production

[pgbouncer]
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25
reserve_pool_size = 5
reserve_pool_timeout = 3
```

**å®è·µå»ºè®®ï¼š**

1. ç´¢å¼•ç­–ç•¥ï¼š
```sql
-- ä¸è¦åˆ›å»ºå†—ä½™ç´¢å¼•
-- âŒ é”™è¯¯ï¼š
CREATE INDEX idx1 ON users(email);
CREATE INDEX idx2 ON users(email, created_at);  -- idx1æ˜¯å†—ä½™çš„

-- âœ… æ­£ç¡®ï¼šåªä¿ç•™å¤åˆç´¢å¼•
CREATE INDEX idx_users_email_created ON users(email, created_at);

-- ä½¿ç”¨INCLUDEåˆ—é¿å…å›è¡¨
CREATE INDEX idx_users_email
ON users(email)
INCLUDE (username, phone);
```

2. æŸ¥è¯¢ä¼˜åŒ–æŠ€å·§ï¼š
```sql
-- ä½¿ç”¨EXISTSä»£æ›¿INï¼ˆå¤§æ•°æ®é›†ï¼‰
-- âŒ æ…¢ï¼š
SELECT * FROM orders WHERE user_id IN (
    SELECT user_id FROM premium_users
);

-- âœ… å¿«ï¼š
SELECT * FROM orders o WHERE EXISTS (
    SELECT 1 FROM premium_users p
    WHERE p.user_id = o.user_id
);
```

3. é…ç½®ä¼˜åŒ–ï¼ˆpostgresql.confï¼‰ï¼š
```ini
# PostgreSQL 16 ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼ˆ16GBå†…å­˜æœåŠ¡å™¨ï¼‰
shared_buffers = 4GB                # ç³»ç»Ÿå†…å­˜çš„25%
effective_cache_size = 12GB         # ç³»ç»Ÿå†…å­˜çš„75%
work_mem = 64MB                     # æ¯ä¸ªæŸ¥è¯¢æ“ä½œçš„å†…å­˜
maintenance_work_mem = 1GB          # ç»´æŠ¤æ“ä½œå†…å­˜
max_connections = 200
random_page_cost = 1.1              # SSDé™ä½æ­¤å€¼
effective_io_concurrency = 200      # SSDå¹¶å‘IO
```

**è¸©å‘ç»éªŒï¼š**

âš ï¸ å‘1ï¼šè¿‡å¤šç´¢å¼•å¯¼è‡´å†™å…¥æ€§èƒ½ä¸‹é™
```sql
-- æŸè¡¨æœ‰12ä¸ªç´¢å¼•ï¼ŒINSERTæ€§èƒ½ä¸‹é™70%
-- åˆ†æç´¢å¼•ä½¿ç”¨ç‡
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
WHERE idx_scan = 0  -- ä»æœªä½¿ç”¨çš„ç´¢å¼•
ORDER BY schemaname, tablename;
```

âš ï¸ å‘2ï¼šç»Ÿè®¡ä¿¡æ¯è¿‡æœŸå¯¼è‡´é”™è¯¯çš„æ‰§è¡Œè®¡åˆ’
```sql
-- æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯æœ€åæ›´æ–°æ—¶é—´
SELECT
    schemaname,
    tablename,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE last_analyze < NOW() - INTERVAL '7 days';
```

âœ… è§£å†³æ–¹æ¡ˆï¼š
```sql
-- å®šæœŸåˆ†æè¡¨
ANALYZE VERBOSE orders;

-- æˆ–è®¾ç½®è‡ªåŠ¨vacuumå’Œanalyze
ALTER TABLE orders SET (
    autovacuum_analyze_scale_factor = 0.05,
    autovacuum_analyze_threshold = 1000
);
```

âš ï¸ å‘3ï¼šN+1æŸ¥è¯¢é—®é¢˜
```javascript
// âŒ é”™è¯¯ï¼šæ¯ä¸ªè®¢å•éƒ½æŸ¥è¯¢ä¸€æ¬¡ç”¨æˆ·
orders.forEach(async (order) => {
    const user = await db.query(
        'SELECT * FROM users WHERE user_id = $1',
        [order.user_id]
    );
});
// 100ä¸ªè®¢å• = 101æ¬¡æŸ¥è¯¢
```

âœ… è§£å†³æ–¹æ¡ˆï¼š
```javascript
// âœ… æ­£ç¡®ï¼šæ‰¹é‡æŸ¥è¯¢
const userIds = orders.map(o => o.user_id);
const users = await db.query(
    'SELECT * FROM users WHERE user_id = ANY($1)',
    [userIds]
);
// 100ä¸ªè®¢å• = 2æ¬¡æŸ¥è¯¢
```

**æ¨èèµ„æºï¼š**

â€¢ å®˜æ–¹æ–‡æ¡£ï¼šPostgreSQL Performance Tuning (https://wiki.postgresql.org/wiki/Performance_Optimization)
â€¢ ä¹¦ç±ï¼šã€ŠPostgreSQL 14 Internalsã€‹- æ·±å…¥ç†è§£æŸ¥è¯¢ä¼˜åŒ–å™¨
â€¢ å·¥å…·ï¼š
  - pg_stat_statementsï¼šæŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
  - pgBadgerï¼šæ—¥å¿—åˆ†æå·¥å…·
  - explain.depesz.comï¼šå¯è§†åŒ–EXPLAINç»“æœ
â€¢ ç›‘æ§ï¼šPrometheus + postgres_exporter + Grafana

**æ€§èƒ½æå‡æ•°æ®ï¼š**
- æŸ¥è¯¢å“åº”æ—¶é—´ï¼š3200ms â†’ 8msï¼ˆ-99.75%ï¼‰
- æ•°æ®åº“CPUä½¿ç”¨ç‡ï¼š90% â†’ 15%ï¼ˆ-75%ï¼‰
- QPSï¼ˆæ¯ç§’æŸ¥è¯¢æ•°ï¼‰ï¼š50 â†’ 2000ï¼ˆ+3900%ï¼‰
- ç£ç›˜IOï¼šå‡å°‘92%

ä½ é‡åˆ°è¿‡ç±»ä¼¼é—®é¢˜å—ï¼Ÿ

---

## ğŸ‡¬ğŸ‡§ English Version

# PostgreSQL Performance Optimization: From Slow Queries to Millisecond Response

Last month we optimized our main query from 3.2 seconds to 8ms - a 400x improvement. This optimization taught me the essence of database performance tuning.

**My Journey:**

Initially I thought adding a few indexes would solve the problem. Instead, queries got even slower.

Then I discovered that index strategy, query plans, statistics, and connection pooling all affect performance - it's a systems engineering problem.

Now my view is: PostgreSQL performance optimization requires understanding how the query executor works, not blindly adding indexes.

**Core Concepts Explained:**

Index Selection Principles:
1. B-Tree: For equality and range queries (default)
2. GIN: For full-text search, JSON queries, arrays
3. GiST: For geospatial data, range types
4. BRIN: For time-series data in large tables

Query Optimizer:
- Cost-Based Optimizer (CBO)
- Uses statistics to estimate row counts
- Selects optimal execution plan

Join Types:
- Nested Loop: Small table drives large table
- Hash Join: Equality joins on large tables
- Merge Join: Joins on sorted data

**Real-world Case:**

Scenario: E-commerce order system querying user orders from last 30 days with product details and payment info.

Problem: Query took 3.2s, severely impacting UX, DB CPU at 90%.

Original Query:
```sql
SELECT
    o.order_id,
    o.created_at,
    u.username,
    u.email,
    p.product_name,
    p.price,
    pay.payment_method,
    pay.paid_at
FROM orders o
JOIN users u ON o.user_id = u.user_id
JOIN order_items oi ON o.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
LEFT JOIN payments pay ON o.order_id = pay.order_id
WHERE o.created_at >= NOW() - INTERVAL '30 days'
AND o.status = 'completed'
ORDER BY o.created_at DESC
LIMIT 100;
```

Optimization Steps:

1. Analyze Query Plan:
```sql
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
[above query];
```

Issues Found:
- Full table scan on orders (Seq Scan), scanning 2M rows
- No index usage
- Hash Join consuming excessive memory

2. Create Composite Indexes:
```sql
-- Cover WHERE and ORDER BY
CREATE INDEX idx_orders_status_created
ON orders(status, created_at DESC)
WHERE status = 'completed';

-- Accelerate JOINs
CREATE INDEX idx_order_items_order_id
ON order_items(order_id)
INCLUDE (product_id);

CREATE INDEX idx_payments_order_id
ON payments(order_id);
```

3. Update Statistics:
```sql
ANALYZE orders;
ANALYZE order_items;
ANALYZE products;
ANALYZE payments;
```

4. Optimize Query (Use CTE):
```sql
WITH recent_orders AS (
    SELECT
        order_id,
        user_id,
        created_at,
        status
    FROM orders
    WHERE status = 'completed'
    AND created_at >= NOW() - INTERVAL '30 days'
    ORDER BY created_at DESC
    LIMIT 100
)
SELECT
    ro.order_id,
    ro.created_at,
    u.username,
    u.email,
    p.product_name,
    p.price,
    pay.payment_method,
    pay.paid_at
FROM recent_orders ro
JOIN users u ON ro.user_id = u.user_id
JOIN order_items oi ON ro.order_id = oi.order_id
JOIN products p ON oi.product_id = p.product_id
LEFT JOIN payments pay ON ro.order_id = pay.order_id
ORDER BY ro.created_at DESC;
```

Result:
- Query time: 3200ms â†’ 8ms (400x improvement)
- Rows scanned: 2,000,000 â†’ 342
- Memory usage: -85%
- CPU utilization: 90% â†’ 15%

**Technical Points:**

â€¢ Use EXPLAIN ANALYZE:
```sql
-- View execution plan and actual timing
EXPLAIN (ANALYZE, BUFFERS, VERBOSE) SELECT ...;

-- Focus on:
-- 1. Seq Scan (full table scan) - needs index
-- 2. rows (estimated vs actual) - stats accuracy
-- 3. buffers (hit rate) - shared_buffers sizing
```

â€¢ Partitioning for Large Data:
```sql
-- Time-based partitioning (PostgreSQL 16)
CREATE TABLE orders (
    order_id BIGINT,
    created_at TIMESTAMP,
    status VARCHAR(20)
) PARTITION BY RANGE (created_at);

CREATE TABLE orders_2025_01 PARTITION OF orders
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

â€¢ Connection Pooling (PgBouncer):
```ini
# pgbouncer.ini
[databases]
mydb = host=localhost port=5432 dbname=production

[pgbouncer]
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25
```

**Practical Advice:**

1. Index Strategy:
```sql
-- Avoid redundant indexes
-- âŒ Wrong:
CREATE INDEX idx1 ON users(email);
CREATE INDEX idx2 ON users(email, created_at);  -- idx1 redundant

-- âœ… Right: Keep only composite
CREATE INDEX idx_users_email_created ON users(email, created_at);

-- Use INCLUDE to avoid table lookup
CREATE INDEX idx_users_email
ON users(email)
INCLUDE (username, phone);
```

2. Query Optimization:
```sql
-- Use EXISTS instead of IN (large datasets)
-- âŒ Slow:
SELECT * FROM orders WHERE user_id IN (
    SELECT user_id FROM premium_users
);

-- âœ… Fast:
SELECT * FROM orders o WHERE EXISTS (
    SELECT 1 FROM premium_users p
    WHERE p.user_id = o.user_id
);
```

3. Configuration (postgresql.conf):
```ini
# PostgreSQL 16 Production (16GB RAM server)
shared_buffers = 4GB                # 25% of RAM
effective_cache_size = 12GB         # 75% of RAM
work_mem = 64MB                     # Per query operation
maintenance_work_mem = 1GB          # Maintenance ops
max_connections = 200
random_page_cost = 1.1              # Lower for SSD
effective_io_concurrency = 200      # SSD concurrent IO
```

**Lessons Learned:**

âš ï¸ Pitfall 1: Too many indexes slow writes
```sql
-- Table with 12 indexes, INSERT performance -70%
-- Analyze index usage
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan
FROM pg_stat_user_indexes
WHERE idx_scan = 0  -- Never used
ORDER BY schemaname, tablename;
```

âš ï¸ Pitfall 2: Stale statistics cause bad plans
```sql
-- Check last analysis time
SELECT
    schemaname,
    tablename,
    last_analyze,
    last_autoanalyze
FROM pg_stat_user_tables
WHERE last_analyze < NOW() - INTERVAL '7 days';
```

âœ… Solution:
```sql
-- Regular analysis
ANALYZE VERBOSE orders;

-- Or configure autovacuum
ALTER TABLE orders SET (
    autovacuum_analyze_scale_factor = 0.05,
    autovacuum_analyze_threshold = 1000
);
```

âš ï¸ Pitfall 3: N+1 Query Problem
```javascript
// âŒ Wrong: One query per order
orders.forEach(async (order) => {
    const user = await db.query(
        'SELECT * FROM users WHERE user_id = $1',
        [order.user_id]
    );
});
// 100 orders = 101 queries
```

âœ… Solution:
```javascript
// âœ… Right: Batch query
const userIds = orders.map(o => o.user_id);
const users = await db.query(
    'SELECT * FROM users WHERE user_id = ANY($1)',
    [userIds]
);
// 100 orders = 2 queries
```

**Recommended Resources:**

â€¢ Official: PostgreSQL Performance Tuning (https://wiki.postgresql.org/wiki/Performance_Optimization)
â€¢ Book: "PostgreSQL 14 Internals" - Deep dive into query optimizer
â€¢ Tools:
  - pg_stat_statements: Query performance stats
  - pgBadger: Log analysis
  - explain.depesz.com: Visualize EXPLAIN
â€¢ Monitoring: Prometheus + postgres_exporter + Grafana

**Performance Improvements:**
- Query response: 3200ms â†’ 8ms (-99.75%)
- DB CPU: 90% â†’ 15% (-75%)
- QPS: 50 â†’ 2000 (+3900%)
- Disk IO: -92%

Have you encountered similar issues?

---

## æ ‡ç­¾ / Tags
#PostgreSQL #æ•°æ®åº“ #Database #æ€§èƒ½ä¼˜åŒ– #Performance #ç¼–ç¨‹ #Programming #SQL

## å‘å¸ƒå»ºè®® / Publishing Tips
- æœ€ä½³æ—¶é—´ / Best Time: å·¥ä½œæ—¥æ—©æ™¨9:00æˆ–ä¸‹åˆ15:00 / Weekday 9AM or 3PM
- é™„å›¾ / Attach: EXPLAINè®¡åˆ’æˆªå›¾ã€æ€§èƒ½å¯¹æ¯”å›¾è¡¨ / EXPLAIN plans, performance charts
- äº’åŠ¨ / Engagement: æŠ€æœ¯è®¨è®ºã€æ€§èƒ½ä¼˜åŒ–ç»éªŒ / Technical discussion, optimization tips
- å¹³å° / Platform: X/Twitter, Dev.to, æ˜é‡‘, DBA StackExchange

## åˆ›ä½œæ—¥æœŸ / Created
2025-12-04
